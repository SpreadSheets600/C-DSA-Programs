# Asymptotic Notations

## **What is Asymptotic Notation?**

Imagine youâ€™re trying to see **how fast an algorithm is** as your input size grows.
You donâ€™t care about small details like:

* The brand of your CPU
* Whether you used `i++` or `i += 1`
* Exact number of milliseconds

Instead, you want to know:
ğŸ’¡ **â€œWhat happens when my input gets HUGE?â€**

Thatâ€™s where **asymptotic notation** comes in. Itâ€™s like **short-hand math** for describing *growth rates* of algorithms.

---

## **Why do we need it?**

1. **Ignore constants and small terms** â€” focus on the big picture.
2. **Compare algorithms fairly** â€” across different machines.
3. **Predict scalability** â€” how well will it handle massive input.

Example:

* Algorithm A takes `0.05nÂ² + 2n + 3` operations.
* Algorithm B takes `100n + 10` operations.

For small `n`, B might be slower due to the big constant `100`.
But as `n` grows, `nÂ²` will crush `n`, so **A will eventually be much slower**.

---

## **The Main Asymptotic Notations**

There are **five primary notations** youâ€™ll see often.

---

### **Big O â€” Upper Bound**

**Definition:** Describes the **worst-case growth rate**.
It tells you: â€œIt wonâ€™t grow faster than this.â€

**Formal idea:**

```bash
f(n) = O(g(n))  means  f(n) â‰¤ C Ã— g(n)   for some constant C, for large enough n.
```

**Example:**
If an algorithm takes `3n + 2` steps, then itâ€™s **O(n)**.
It wonâ€™t ever grow faster than some multiple of `n`.

**You use Big O for:**

* Worst-case runtime (common in competitive programming).
* Space complexity upper bound.

---

### **Big Omega (Î©) â€” Lower Bound**

**Definition:** Describes the **best-case growth rate**.
It says: â€œIt will take at least this much time.â€

**Formal idea:**

```bash
f(n) = Î©(g(n))  means  f(n) â‰¥ C Ã— g(n)   for some constant C, for large enough n.
```

**Example:**
If you must check every element at least once, best case is **Î©(n)**.

---

### **Big Theta (Î˜) â€” Tight Bound**

**Definition:** Describes an **exact growth rate**.
It says: â€œIt grows at this rate, no faster, no slower (asymptotically).â€

**Formal idea:**

```bash
f(n) = Î˜(g(n))  means  itâ€™s both O(g(n)) and Î©(g(n)).
```

**Example:**
`5n + 3` is Î˜(n) because:

* Upper bound is O(n)
* Lower bound is Î©(n)

**Think:** Î˜ is like **pinning it down exactly**.

---

### **Little o â€” Strictly Less Than**

**Definition:** Describes that a function grows **strictly slower** than another.

**Formal idea:**

```bash
f(n) = o(g(n))  means  f(n) grows slower than g(n), and ratio â†’ 0 as n â†’ âˆ.
```

**Example:**
`n` is `o(nÂ²)` because as n grows, `n / nÂ² = 1/n â†’ 0`.

---

### **Little omega â€” Strictly Greater Than**

**Definition:** Opposite of little o.
Function grows **strictly faster**.

**Formal idea:**

```bash
f(n) = Ï‰(g(n))  means  f(n) grows faster than g(n), and ratio â†’ âˆ as n â†’ âˆ.
```

**Example:**
`nÂ²` is `Ï‰(n)` because `nÂ² / n = n â†’ âˆ`.

---

![Growth Chart](../.github/AsymptoticNotation.png)

---

## **Order of Growth Cheat Sheet**

From slowest to fastest (common ones):

| Growth Rate    | Name          | Example                        |
| -------------- | ------------- | ------------------------------ |
| **O(1)**       | Constant time | Access array element           |
| **O(log n)**   | Logarithmic   | Binary search                  |
| **O(n)**       | Linear        | Loop through array             |
| **O(n log n)** | Log-linear    | Merge sort                     |
| **O(nÂ²)**      | Quadratic     | Nested loops                   |
| **O(nÂ³)**      | Cubic         | Triple nested loops            |
| **O(2â¿)**      | Exponential   | Naive recursive Fibonacci      |
| **O(n!)**      | Factorial     | Traveling salesman brute-force |

---

## **Dropping Constants & Lower-Order Terms**

When writing Big O, we **only care about the dominant term** as `n â†’ âˆ`.

Example:
`T(n) = 3nÂ² + 10n + 5` â†’ **O(nÂ²)**
Because:

* `nÂ²` dominates for large `n`
* Constants (3, 10, 5) donâ€™t matter.

---

## **Best, Worst, and Average Cases**

* **Best case** â†’ Often given in Î© notation.
* **Worst case** â†’ Often given in O notation.
* **Average case** â†’ Can also use Î˜ notation.

Example:
Linear search in an array of size `n`:

* Best case (element at start) â†’ Î©(1)
* Worst case (element at end or absent) â†’ O(n)
* Average case â†’ Î˜(n)

---

## **Graphical Intuition**

If we plotted time vs input size:

* **O(n)** â†’ straight upward slope
* **O(log n)** â†’ rises quickly at first, then flattens
* **O(nÂ²)** â†’ curve gets steep fast
* **O(2â¿)** â†’ skyrockets almost immediately

---

## **Quick Summary Table**

| Notation    | Meaning       | Says What?                | Example |
| ----------- | ------------- | ------------------------- | ------- |
| **O(g(n))** | Upper bound   | Worst-case                | O(nÂ²)   |
| **Î©(g(n))** | Lower bound   | Best-case                 | Î©(n)    |
| **Î˜(g(n))** | Tight bound   | Exact asymptotic behavior | Î˜(n)    |
| **o(g(n))** | Strictly less | Grows slower              | o(nÂ²)   |
| **Ï‰(g(n))** | Strictly more | Grows faster              | Ï‰(n)    |
